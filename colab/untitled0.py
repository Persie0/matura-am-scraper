# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10kdqpZ9nUYCwRd-ZRsdo72chA3dpu4W9
"""

from google.colab import drive

drive.mount("/content/drive")

import requests
import json
from bs4 import BeautifulSoup
import time

URL = "https://prod.aufgabenpool.at/srdp/phpcode/search.php"


all=[]
#loop through a to z
for i in range(97, 123):
#post request with query=buchstabe as payload
    r = requests.post(url = URL, data = {'query': chr(i)})
    #extracting response text
    soup = BeautifulSoup(r.content, "html.parser")
    #get all <li> tags
    li = soup.find_all('li')

    
    #get "id" attribute of the <div> in <li> tag
    li_id = [i.find('div')['id'] for i in li]

    #get inner text of <li> tags
    li_text = [i.text for i in li]

    #remove text between and "<" until ">" 
    li_text = [i.split("<")[0] for i in li_text]

    #remove "(number)" at the end
    li_text = [i.split("(")[0] for i in li_text]

    #trim whitespaces
    li_text = [i.strip() for i in li_text]

    #combine id and text to json
    li_text = [{"id": li_id[i], "thema": li_text[i]} for i in range(len(li_id))]

    all += li_text
    print(len(all))
    time.sleep(0.2)

#remove duplicates
all = list({v['id']:v for v in all}.values())

#remove {"id": "Ooops ...", "thema": "Ihre Suche hat leider keine Treffer erzielt."}
all = [i for i in all if i['id'] != "Ooops ..."]

#save as json
with open('/content/drive/MyDrive/themen.json', 'w') as outfile:
    json.dump(all, outfile)

import requests
import json
from bs4 import BeautifulSoup
import time

URL = "https://aufgabenpool.at/srdp/phpcode/lp_results.php"

#read themen.json
with open('/content/drive/MyDrive/themen.json') as json_file:
    data = json.load(json_file)

res=[]
for el in data:
    payload = {'id': el['id'], 'coll': ''}

    #post request
    r = requests.post(url = URL, data = payload)

    #extracting response text
    soup = BeautifulSoup(r.content, "html.parser")

    #extract all <li> tags
    li = soup.find_all('li')

    #pdf is <a> tag
    #images can be found with src attribute from <img> tag

    all_imgs = []
    for i in li:
        a_tag = i.find('a')
        if a_tag:
          temp= {'loesung': "https://aufgabenpool.at/srdp/"+a_tag['href']}
        else:
          print(i)
          continue
        #get string between last / and .pdf
        temp['name'] = i.find('a')['href'].split("/")[-1].split(".pdf")[0]
        #find all <img> tags
        imgs = i.find_all('img')
        for i in range(len(imgs)):
            if i == 0:
                temp['aufgabe_head'] = "https://aufgabenpool.at/srdp/"+imgs[i]['src']
                temp['teil'] = imgs[i]['src'].split("teil")[1].split("/")[0].upper()
                temp['id'] = imgs[i]['src'].split("/")[1].split("/")[0].upper()
                temp["thema"]=el['thema']
            elif i == 1:
                temp['aufgabe_body'] = "https://aufgabenpool.at/srdp/"+imgs[i]['src']
                break
        res.append(temp)
    print(len(res))
    time.sleep(10)

#save as json
with open('/content/drive/MyDrive/beispiele.json', 'w') as outfile:
    json.dump(res, outfile)

!pip install PyPDF2 pdf2image pdfminer

!pip install easyocr

!sudo apt-get install poppler-utils

"""restart runtime"""

import json
import sys
sys.path.append('/usr/local/lib/python3.8/dist-packages')
import requests
from PIL import Image
from io import BytesIO
import PyPDF2
import os
from pdf2image import convert_from_path
import re
from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LAParams
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdf2image import convert_from_path
from PIL import Image

# Loop through the pages of the PDF
def find_right_page(pdf_reader, such_string, first_match=True):
    # Initialize a variable to keep track of the number of times the string is found
    count = 0
    # Iterate through all pages
    for page_number in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_number]
        # Extract the text from the page
        text = page.extract_text()
        # Search for the string in the text
        match = re.search(such_string, text)
        if match:
            if first_match:
                return page_number+1
            else:
                count += 1
                if count == 2:
                    return page_number+1
    return -1
    
def get_string_position_and_crop(pdf_file, string, page_number):
    with open(pdf_file, 'rb') as f:
        # Create a PDF resource manager object that stores shared resources
        resource_manager = PDFResourceManager()
        # Create a PDF page aggregator object
        device = PDFPageAggregator(resource_manager, laparams=LAParams())
        # Create a PDF interpreter object
        interpreter = PDFPageInterpreter(resource_manager, device)
        # Process the specific page
        for i, page in enumerate(PDFPage.get_pages(f)):
            if i == page_number-1:
                interpreter.process_page(page)
                # Receive the LTPage object for the page
                layout = device.get_result()
                # Iterate through the layout elements
                firs=True
                pz=1
                images = convert_from_path(pdf_file, first_page=page_number, last_page=page_number)
                image = images[0]
                p2=image.height
                for element in layout:
                    if hasattr(element,'get_text'):
                        if string in element.get_text().strip():
                            print(element.get_text().strip())
                            if firs:
                                firs=False
                                pz = element.bbox[1]/layout.height
                                # Convert the page to an image
                                
                                
                            else:
                                p2 = element.bbox[1]/layout.height
                                p2=image.height-p2*image.height-50


                            # Crop the image based on the y value
                image = image.crop((0, image.height-pz*image.height-90, image.width, p2))
                return image
    return None

#open themen.json
with open("/content/drive/MyDrive/"+'themen.json') as json_file:
    themen = json.load(json_file)

if not os.path.exists("/content/drive/MyDrive/Themen/"):
  os.makedirs("/content/drive/MyDrive/Themen/")

for the in themen:
  #open beispiele.json
  with open("/content/drive/MyDrive/"+'beispiele.json') as json_file:
      data = json.load(json_file)
  thema=the["thema"]
  print(thema)


  #filter by thema
  data = [i for i in data if i['thema'] == thema]


      #if no thema folder make one
  if not os.path.exists("/content/drive/MyDrive/pdfs"):
      os.makedirs("/content/drive/MyDrive/pdfs")


  for bsp in data:
      aufgabe_name= bsp["name"]+"_"+bsp["id"]

      auf_dir="/content/drive/MyDrive/Themen/"+thema+"/"+aufgabe_name

      if os.path.exists(auf_dir+"/"+bsp["id"]+"loesung.jpeg"):
          continue
      
      #get "aufgabe_body"
      ab=bsp['aufgabe_body']

      #get ab as image and ocr image
      response = requests.get(ab)
      img = Image.open(BytesIO(response.content))
      
      #download "loesung" as pdf
      loe=bsp['loesung']
      pdf_name= aufgabe_name+".pdf"
      #check if already downloaded
      if not os.path.exists("/content/drive/MyDrive/"+"pdfs/"+pdf_name):
          r = requests.get(loe, allow_redirects=True)
          open("/content/drive/MyDrive/"+"pdfs/"+pdf_name, 'wb').write(r.content)

      #extract bsp letter + ")"
      aufgabe = ab.split("/")[-1].split(".")[0]


      pdf_file = open("/content/drive/MyDrive/"+"pdfs/"+pdf_name, "rb")
      pdf_reader = PyPDF2.PdfReader(pdf_file)


      #make aufgabe_name directory in thema dir
      if not os.path.exists(auf_dir):
          os.makedirs(auf_dir)
      
      #download "aufgabe_head" as image
      ah=bsp['aufgabe_head']
      ah_name= bsp["id"]+"ah"
      ah_path= auf_dir+"/"+ah_name+".png"
      #check if already downloaded
      if not os.path.exists(ah_path):
          response = requests.get(ah)
          img = Image.open(BytesIO(response.content))
          img.save(ah_path)

      #download "aufgabe_body" as image
      ab=bsp['aufgabe_body']
      ab_name= bsp["id"]+"ab"
      ab_path= auf_dir+"/"+ab_name+".png"
      #check if already downloaded
      if not os.path.exists(ab_path):
          response = requests.get(ab)
          img = Image.open(BytesIO(response.content))
          img.save(ab_path)
      
      #extract solution

      such_string = aufgabe+"1)"
      reg=aufgabe+"1\)"
      print("reg: "+reg)
      such_page = None



      
      print(pdf_file)
      such_page = find_right_page(pdf_reader, reg, True)


      if such_page == -1:
          such_string = aufgabe+")"
          reg=aufgabe+"\)"
          print("reg): "+reg)
          such_page = find_right_page(pdf_reader, reg, False)

      if such_page == -1:
        such_page=len(pdf_reader.pages)-1


      print("Page: " + str(such_page))

          

      image = get_string_position_and_crop("/content/drive/MyDrive/"+"pdfs/"+pdf_name, such_string, such_page)
      if image:
          # Save the cropped image
          image.save(auf_dir+"/"+bsp["id"]+"loesung.jpeg")
          
      pdf_file.close()

!pip install genanki

import json
import genanki


#open beispiele.json
with open('/content/drive/MyDrive/beispiele.json') as json_file:
    data = json.load(json_file)

thema="Binomialverteilung"

my_deck = genanki.Deck(
    deck_id=12345, # a unique id for the deck
    name=thema+"_RDP_scraped") # the name of the deck

img=[]
#filter by thema
data = [i for i in data if i['thema'] == thema]
c=0
for bsp in data:
    c+=1
    aufgabe_name= bsp["name"]+"_"+bsp["id"]
    auf_dir="/content/drive/MyDrive/Themen/"+thema+"/"+aufgabe_name
    ah=bsp['aufgabe_head']
    ah_name= bsp["id"]+"ah"
    ah_path= auf_dir+"/"+ah_name+".png"
    ab=bsp['aufgabe_body']
    ab_name= bsp["id"]+"ab"
    ab_path= auf_dir+"/"+ab_name+".png"
    my_card = genanki.Note(
    model=genanki.Model(
        1380120064,
  'Example',
        fields=[
            {'name': 'Image1'},
            {'name': 'Image2'},
            
            {'name': 'AnswerImage'},
            {'name': 'Answer'},
        ],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '{{Image1}}<br>{{Image2}}',
                'afmt': '{{AnswerImage}}<br>{{Answer}}',
            },
        ]
    ),
    fields=['<img src="{}">'.format(ah_name+".png"), '<img src="{}">'.format(ab_name+".png"), '<img src="{}">'.format(bsp["id"]+"loesung.jpeg"), '<a href="{}">Link</a>'.format(bsp["loesung"])])

    img.append(ah_path)
    img.append(ab_path)
    img.append(auf_dir+"/"+bsp["id"]+"loesung.jpeg")
    my_deck.add_note(my_card)

my_package = genanki.Package(my_deck)
my_package.media_files = img

#create directory if not exists
if not os.path.exists("/content/drive/MyDrive/AM_RDP_Decks/"+thema):
    os.makedirs("/content/drive/MyDrive/AM_RDP_Decks/"+thema)


my_package.write_to_file("/content/drive/MyDrive/AM_RDP_Decks/"+thema+'.apkg')